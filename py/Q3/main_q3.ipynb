{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d8be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 19:16:29.500350: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-06 19:16:29.708590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-06 19:16:31.010283: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import sys,os \n",
    "sys.path.append('/home/benr/ACT/CW2/py')\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functions import get_data,split_data_torch,evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfe2d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "CUDA_VISIBLE_DEVICES: None\n"
     ]
    }
   ],
   "source": [
    "#Ensure CUDA is available. \n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b027a04",
   "metadata": {},
   "source": [
    "# Question 3, Using Data Augmentation to improve a CNN \n",
    "\n",
    "In this Notebook we will explore how image roatation and normalising image data can imporve the Convolutional neural network we created in Q2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ded9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/benr/.astroNN/datasets/Galaxy10_DECals.h5 was found!\n",
      "torch.Size([17736, 256, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "# get the data from functions.py\n",
    "images,labels = get_data()\n",
    "# convert images to pytorch tnesors\n",
    "images = torch.from_numpy(images)\n",
    "labels = torch.from_numpy(labels).long()\n",
    "\n",
    "print(images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24745340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17736, 3, 256, 256])\n",
      "tensor([0.1675, 0.1626, 0.1589])\n",
      "tensor([0.1287, 0.1180, 0.1116])\n"
     ]
    }
   ],
   "source": [
    "# Move channels: (N,H,W,C) -> (N,C,H,W)\n",
    "images = images.permute(0, 3, 1, 2)      \n",
    "# standardise \n",
    "images = images.float() / 255.0    \n",
    "# The mean and standard deviation will be used later for normalisation   \n",
    "mean = images.mean(dim=[0,2,3])\n",
    "std = images.std(dim=[0,2,3])\n",
    "print(images.shape)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993d68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Galaxy_CNN(nn.Module):\n",
    "    def __init__(self, inCH, nCL):  \n",
    "        super(Galaxy_CNN, self).__init__()\n",
    "        # conv blocks\n",
    "        self.conv1 = nn.Conv2d(3, inCH, kernel_size=7, stride=1, padding=4)\n",
    "        # batchnorm is a way of normalising the activation values\n",
    "        self.bn1   = nn.BatchNorm2d(inCH)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(inCH, inCH*2, kernel_size=5, stride=2, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(inCH*2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(inCH*2, inCH, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(inCH)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #dropout is a way of 'turning off' some nodes temporarily, to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        # fully connected final layer \n",
    "        self.lin = nn.Linear(inCH*16**2, nCL)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward section passes the batch through the graph\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))# conv1 -> batchnorm -> relu -> pooling\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))# conv2 -> batchnorm -> relu -> pooling \n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))# conv3 -> batchnorm -> relu -> pooling\n",
    "        x = self.dropout(x) \n",
    "        #change shpae of tensor for linear layer (batch size,total features)    \n",
    "        x = x.view(x.size(0), -1)    \n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe1137",
   "metadata": {},
   "source": [
    "The kinds of data agumentation we want to implement take place in the training function, so for this question we will define the training function in this notebook rather than in functions.py, \n",
    "\n",
    "Since the evaluate() function is unchanged this will still be called from functions.py\n",
    "\n",
    "This code block also includes the random_rotate function, this function will implement a rotatrion on a certrain percentage of the image chosen by the user. The function carries out rotations of $90^o$, $180^o$ or $270^o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c624d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to rotate some images at 90 degree angles  \n",
    "def random_rotate(imgs,p):\n",
    "   # only rotate images half of the time \n",
    "   if torch.rand(1).item() > p:\n",
    "      return imgs \n",
    "   s = imgs.size(0)\n",
    "   n90 = torch.randint(1,4,(s,), device=imgs.device)\n",
    "   rotated_img = []\n",
    "   for img,k in zip(imgs,n90):\n",
    "      rotated_img.append(torch.rot90(img,k = int(k),dims=(1,2)))\n",
    "   return torch.stack(rotated_img,dim =0) \n",
    "\n",
    "def train(model,trainLoader,testLoader,mean_tensor,std_tensor,criterion,optimizer\n",
    "          ,device,n_epoch):\n",
    "  early_stop = 10 \n",
    "  stop = 0\n",
    "  best_acc = 0.0 \n",
    "  for e in range(n_epoch):\n",
    "    #use model in training mode \n",
    "    model.train()\n",
    "    #increment loss for each image \n",
    "    running_loss = 0.0\n",
    "    # number of correct labels\n",
    "    correct = 0\n",
    "    # total number of samples \n",
    "    total = 0\n",
    "\n",
    "\n",
    "    for imgs,lbl in trainLoader:\n",
    "      # move image to GPU\n",
    "      imgs = imgs.to(device)\n",
    "      imgs = (imgs - mean_tensor) / std_tensor \n",
    "      #rotate images at random\n",
    "      imgs = random_rotate(imgs,0.5)\n",
    "      #move label to GPU \n",
    "      labels = lbl.to(device)\n",
    "      #clear old gradients \n",
    "      optimizer.zero_grad()\n",
    "      # predict labels \n",
    "      outputs = model(imgs)\n",
    "      # workout the loss for the prediction \n",
    "      loss = criterion(outputs,labels)\n",
    "      # use backpropagation to calculate new weights \n",
    "      loss.backward()\n",
    "      # update weights \n",
    "      optimizer.step()\n",
    "      # get batch loss \n",
    "      running_loss += loss.item() * imgs.size(0)\n",
    "      # return position of most likely catagory for each image in batch \n",
    "      _,preds = torch.max(outputs,1)\n",
    "      # count the correctly predicted samples \n",
    "      correct += (preds == labels).sum().item()\n",
    "      # increae the total samples \n",
    "      total += labels.size(0)\n",
    "    # calculate train_loss for epoch \n",
    "    trn_loss = running_loss / total\n",
    "    # calculate train accuracy \n",
    "    trn_acc = correct / total \n",
    "    # test trained model \n",
    "    test_acc = evaluate(model, testLoader,mean_tensor, std_tensor, device)\n",
    "    if test_acc > best_acc:\n",
    "       best_acc = test_acc\n",
    "       torch.save(model.state_dict(), \"best_model.pt\")\n",
    "       print(f\"Saved new best model (acc={best_acc:.4f})\")\n",
    "       stop = 0 \n",
    "    else:\n",
    "       stop += 1 \n",
    "    if stop == early_stop:\n",
    "      break \n",
    "    print(f\"Epoch {e+1}/{n_epoch} \"\n",
    "        f\"Train loss: {trn_loss:.4f}  \"\n",
    "        f\"Train acc: {trn_acc:.3f}  \"\n",
    "        f\"Test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa9a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split tensor into training and test sets (see functions.py)\n",
    "I = np.arange(len(labels))\n",
    "xtrn,xtst,ytrn,ytst = split_data_torch(images,labels,I)\n",
    "\n",
    "\n",
    "# match images and labels in a torch dataset \n",
    "train_ds = TensorDataset(xtrn, ytrn)\n",
    "test_ds  = TensorDataset(xtst, ytst)\n",
    "\n",
    "\n",
    "\n",
    "#create train and test loaders for training\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=5, shuffle=True,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader  = DataLoader(\n",
    "    test_ds, batch_size=5, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ead91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# number of catagories \n",
    "num_classes = len(labels.unique())\n",
    "# tell the computer to use the GPU via CUDA \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#The network model \n",
    "model = Galaxy_CNN(128,num_classes).to(device)\n",
    "\n",
    "''' The criterion is the loss function, in this case we are using cross entropy. \n",
    "This function asigns probabilities to each catagory, for example [0,1,0,0...] might be\n",
    "a spiral galaxy. Cross entropy then measures how close the outputs are to this true \n",
    "probability.'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# the method of gradient decent used in backpropogation. \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=10**-3,weight_decay=1e-4)\n",
    "\n",
    "#for normalisation \n",
    "mean_tensor = mean.view(1,3,1,1).to(device)\n",
    "std_tensor = std.view(1,3,1,1).to(device)\n",
    "\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "964eb9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model (acc=0.4622)\n",
      "Epoch 1/70 Train loss: 2.2041  Train acc: 0.342  Test acc: 0.462\n",
      "Saved new best model (acc=0.5190)\n",
      "Epoch 2/70 Train loss: 1.4919  Train acc: 0.464  Test acc: 0.519\n",
      "Saved new best model (acc=0.5626)\n",
      "Epoch 3/70 Train loss: 1.3665  Train acc: 0.513  Test acc: 0.563\n",
      "Saved new best model (acc=0.6099)\n",
      "Epoch 4/70 Train loss: 1.2705  Train acc: 0.552  Test acc: 0.610\n",
      "Epoch 5/70 Train loss: 1.1633  Train acc: 0.591  Test acc: 0.608\n",
      "Saved new best model (acc=0.6467)\n",
      "Epoch 6/70 Train loss: 1.0854  Train acc: 0.621  Test acc: 0.647\n",
      "Saved new best model (acc=0.6749)\n",
      "Epoch 7/70 Train loss: 1.0171  Train acc: 0.647  Test acc: 0.675\n",
      "Epoch 8/70 Train loss: 0.9635  Train acc: 0.670  Test acc: 0.668\n",
      "Saved new best model (acc=0.7076)\n",
      "Epoch 9/70 Train loss: 0.9212  Train acc: 0.684  Test acc: 0.708\n",
      "Epoch 10/70 Train loss: 0.8914  Train acc: 0.698  Test acc: 0.697\n",
      "Epoch 11/70 Train loss: 0.8501  Train acc: 0.715  Test acc: 0.703\n",
      "Saved new best model (acc=0.7151)\n",
      "Epoch 12/70 Train loss: 0.8268  Train acc: 0.723  Test acc: 0.715\n",
      "Epoch 13/70 Train loss: 0.8054  Train acc: 0.726  Test acc: 0.703\n",
      "Saved new best model (acc=0.7178)\n",
      "Epoch 14/70 Train loss: 0.7952  Train acc: 0.735  Test acc: 0.718\n",
      "Saved new best model (acc=0.7309)\n",
      "Epoch 15/70 Train loss: 0.7788  Train acc: 0.740  Test acc: 0.731\n",
      "Saved new best model (acc=0.7430)\n",
      "Epoch 16/70 Train loss: 0.7559  Train acc: 0.747  Test acc: 0.743\n",
      "Epoch 17/70 Train loss: 0.7494  Train acc: 0.748  Test acc: 0.735\n",
      "Epoch 18/70 Train loss: 0.7414  Train acc: 0.752  Test acc: 0.734\n",
      "Epoch 19/70 Train loss: 0.7306  Train acc: 0.753  Test acc: 0.733\n",
      "Epoch 20/70 Train loss: 0.7102  Train acc: 0.759  Test acc: 0.735\n",
      "Saved new best model (acc=0.7437)\n",
      "Epoch 21/70 Train loss: 0.7042  Train acc: 0.762  Test acc: 0.744\n",
      "Epoch 22/70 Train loss: 0.7060  Train acc: 0.766  Test acc: 0.727\n",
      "Saved new best model (acc=0.7452)\n",
      "Epoch 23/70 Train loss: 0.7000  Train acc: 0.768  Test acc: 0.745\n",
      "Epoch 24/70 Train loss: 0.6863  Train acc: 0.767  Test acc: 0.737\n",
      "Saved new best model (acc=0.7520)\n",
      "Epoch 25/70 Train loss: 0.6833  Train acc: 0.770  Test acc: 0.752\n",
      "Epoch 26/70 Train loss: 0.6745  Train acc: 0.776  Test acc: 0.740\n",
      "Epoch 27/70 Train loss: 0.6691  Train acc: 0.775  Test acc: 0.744\n",
      "Epoch 28/70 Train loss: 0.6601  Train acc: 0.775  Test acc: 0.741\n",
      "Epoch 29/70 Train loss: 0.6586  Train acc: 0.780  Test acc: 0.742\n",
      "Epoch 30/70 Train loss: 0.6482  Train acc: 0.779  Test acc: 0.722\n",
      "Epoch 31/70 Train loss: 0.6427  Train acc: 0.779  Test acc: 0.740\n",
      "Epoch 32/70 Train loss: 0.6462  Train acc: 0.780  Test acc: 0.743\n",
      "Saved new best model (acc=0.7580)\n",
      "Epoch 33/70 Train loss: 0.6441  Train acc: 0.786  Test acc: 0.758\n",
      "Epoch 34/70 Train loss: 0.6362  Train acc: 0.787  Test acc: 0.742\n",
      "Epoch 35/70 Train loss: 0.6378  Train acc: 0.783  Test acc: 0.750\n",
      "Epoch 36/70 Train loss: 0.6196  Train acc: 0.791  Test acc: 0.729\n",
      "Epoch 37/70 Train loss: 0.6234  Train acc: 0.789  Test acc: 0.749\n",
      "Epoch 38/70 Train loss: 0.6104  Train acc: 0.791  Test acc: 0.749\n",
      "Epoch 39/70 Train loss: 0.6124  Train acc: 0.790  Test acc: 0.741\n",
      "Saved new best model (acc=0.7595)\n",
      "Epoch 40/70 Train loss: 0.6090  Train acc: 0.794  Test acc: 0.759\n",
      "Epoch 41/70 Train loss: 0.6027  Train acc: 0.795  Test acc: 0.754\n",
      "Epoch 42/70 Train loss: 0.6015  Train acc: 0.799  Test acc: 0.750\n",
      "Epoch 43/70 Train loss: 0.5941  Train acc: 0.800  Test acc: 0.759\n",
      "Epoch 44/70 Train loss: 0.6023  Train acc: 0.797  Test acc: 0.739\n",
      "Epoch 45/70 Train loss: 0.5922  Train acc: 0.801  Test acc: 0.728\n",
      "Saved new best model (acc=0.7621)\n",
      "Epoch 46/70 Train loss: 0.6068  Train acc: 0.795  Test acc: 0.762\n",
      "Epoch 47/70 Train loss: 0.5916  Train acc: 0.803  Test acc: 0.756\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# begin training \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmean_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstd_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m      \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m70\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, trainLoader, testLoader, mean_tensor, std_tensor, criterion, optimizer, device, n_epoch)\u001b[39m\n\u001b[32m     46\u001b[39m optimizer.step()\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# get batch loss \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * imgs.size(\u001b[32m0\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# return position of most likely catagory for each image in batch \u001b[39;00m\n\u001b[32m     50\u001b[39m _,preds = torch.max(outputs,\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# begin training \n",
    "train(model, train_loader,test_loader,  \n",
    "      mean_tensor,std_tensor,\n",
    "      criterion, \n",
    "      optimizer, device, \n",
    "      n_epoch=70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
