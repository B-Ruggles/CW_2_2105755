{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05015e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os \n",
    "sys.path.append('/home/benr/ACT/CW2/py')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functions import get_data, galaxy_type,split_data_torch,train\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab2646",
   "metadata": {},
   "source": [
    "# Question 2, Convolutional Neural Network for image recognition \n",
    "In this section the aim is to repeat the same process as question 1 but replace the traditional reandom forest method with a convolutional neural network. The expectation is that due to the CNN's ability to correlate spacial features, it will be a more roubust method of recognising galaxy morphology.\n",
    "\n",
    "The first step will be to load the data in exactly the same way as Q1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f9468cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the data from functions.py\n",
    "df = get_data()\n",
    "#The following ids will be used to identify each galaxy image from SDSS\n",
    "RA_COL, DEC_COL, ID_COL = 'ra','dec','dr7objid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29a2d42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 231)\n"
     ]
    }
   ],
   "source": [
    "#just incase get rid of any nan cells\n",
    "df_clean = df.dropna(subset=[RA_COL,DEC_COL,ID_COL])\n",
    "#cut data to 15,000\n",
    "df_subset = df_clean.sample(n=15000,random_state=11)\n",
    "#print the shape\n",
    "print(df_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "713a0ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hubble_class\n",
       "Spiral    5894\n",
       "E         3366\n",
       "Disk      2436\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choose best labels to classify galaxies \n",
    "labels = ['t01_smooth_or_features_a01_smooth_debiased',\n",
    "          't01_smooth_or_features_a02_features_or_disk_debiased',\n",
    "          't02_edgeon_a04_yes_debiased',\n",
    "          't04_spiral_a08_spiral_debiased',\n",
    "          't03_bar_a06_bar_debiased'\n",
    "          ] \n",
    "\n",
    "\n",
    "'''create a new column called 'hubble class' and use the above\n",
    "funtion to get the class for each row''' \n",
    "\n",
    "df_subset['hubble_class'] = df_subset.apply(galaxy_type, axis=1)\n",
    "df_subset = df_subset[df_subset['hubble_class'].notna()]\n",
    "df_subset['hubble_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0752f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11693, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# input image data into random forest\n",
    "# attach lables for each image\n",
    "x_images = []\n",
    "y_labels = []\n",
    "\n",
    "\n",
    "# folder that stores the images \n",
    "img_dir = '/home/benr/ACT/CW2/sdss_images'\n",
    "# itterate through each galaxy in the set \n",
    "for idx,row in df_subset.iterrows():\n",
    "    obid = row[ID_COL]\n",
    "    lbl = row['hubble_class']\n",
    "    #find corrosponding galaxy in the image folder \n",
    "    img_path = os.path.join(img_dir, f\"{obid}.jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        continue \n",
    "    try:\n",
    "        #use this error incase of currupted images (optional)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "    except UnidentifiedImageError:\n",
    "        print(\"Skipping corrupted image:\", img_path)\n",
    "        continue\n",
    "    #store image as numpy array \n",
    "    px_arr = np.array(img,dtype = np.float32) / 255.0 \n",
    "    #convert to torch tesnsor \n",
    "    px_tns = torch.tensor(px_arr).unsqueeze(0)\n",
    "    #append image and label to x_images and y_labels \n",
    "    x_images.append(px_tns)\n",
    "    y_labels.append(lbl)\n",
    "# stack images. tensor shape should be (N,1,256,256)\n",
    "x_images = torch.stack(x_images)\n",
    "m = x_images.mean()\n",
    "std = x_images.std()\n",
    "x_images = (x_images - m) / std\n",
    "print(x_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11369fd2",
   "metadata": {},
   "source": [
    "For the CNN the lables will need to be encoded into numerical values. \n",
    "Documentation can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "694512c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# define encoder \n",
    "lb_enc = LabelEncoder()\n",
    "#transform the current y_labels list\n",
    "y_labels = lb_enc.fit_transform(y_labels)\n",
    "#convert list into a torch tensor \n",
    "y_labels = torch.tensor(y_labels) \n",
    "print(len(y_labels.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c2a76",
   "metadata": {},
   "source": [
    "Now we set up the class that will hold the structure of the network. Galaxy_CNN takes the number of input channels, the number of classes (Prediction catagories) and passes them two the different layers of the network. \n",
    "Throughout the process of data being passed through different levels of the CNN we will need to keep trach of the network is change the size of each tensor. This can be done with the following formulas\n",
    "\n",
    "* CNN Output - $$\\frac{H + 2P -K}{s} + 1 $$\n",
    "* Pooling Output - $$\\frac{H-K}{s}  + 1$$\n",
    "\n",
    "H = input height of width of image \n",
    "\n",
    "\n",
    "K = kernel size\n",
    "\n",
    "P = padding size\n",
    "\n",
    "s = stride "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c59235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Galaxy_CNN(nn.Module):\n",
    "    def __init__(self,inCH, nCL):  \n",
    "        \n",
    "        super(Galaxy_CNN,self).__init__()\n",
    "        #first convolution layer\n",
    "        self.conv1 = nn.Conv2d(1, inCH,kernel_size= 3,stride=1,padding =2)\n",
    "        #first polling layer\n",
    "        self.conv2 = nn.Conv2d(inCH,inCH*3, kernel_size=6,stride = 1,padding=2)\n",
    "        #linear output layer\n",
    "        self.lin = nn.Linear(inCH*3 * 64**2,nCL)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f8fe795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split tensor into training and test sets (see functions.py)\n",
    "I = np.arange(len(y_labels))\n",
    "xtrn,xtst,ytrn,ytst = split_data_torch(x_images,y_labels,I)\n",
    "\n",
    "train_ds = TensorDataset(xtrn, ytrn)\n",
    "test_ds  = TensorDataset(xtst, ytst)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ffd8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(y_labels.unique())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Galaxy_CNN(64,num_classes).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(),lr=10**-4,weight_decay=0.00001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5abbf42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60 Train loss: 0.9436  Train acc: 0.585  Test acc: 0.645\n",
      "Epoch 2/60 Train loss: 0.7440  Train acc: 0.689  Test acc: 0.625\n",
      "Epoch 3/60 Train loss: 0.6210  Train acc: 0.742  Test acc: 0.650\n",
      "Epoch 4/60 Train loss: 0.4633  Train acc: 0.819  Test acc: 0.627\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ACT/CW2/py/functions.py:73\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, trainLoader, testLoader, criterion, optimizer, device, n_epoch)\u001b[39m\n\u001b[32m     71\u001b[39m optimizer.step()\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# get batch loss \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * imgs.size(\u001b[32m0\u001b[39m)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# return position of most likely catagory for each image in batch \u001b[39;00m\n\u001b[32m     75\u001b[39m _,preds = torch.max(outputs,\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, criterion, optimizer, device, n_epoch=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e211ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee2b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
